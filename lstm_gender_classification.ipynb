{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pretrained glove embeddings to embed words\n",
    "embeddings_index = {}\n",
    "embedding_dim = 100 #change to other dim as well\n",
    "f = open(os.path.join('./glove.6B', 'glove.6B.100d.txt')) #TODO try 300 dimensions\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing xml files to x train and y train data\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "path = './en/'\n",
    "all_texts = []\n",
    "y_train = []\n",
    "x_train = []\n",
    "max_text_length = 500\n",
    "for filename in os.listdir(path):    \n",
    "    root = ET.parse(path + filename).getroot()\n",
    "    #TODO add for other classifications, ie. age_group and multi-class \n",
    "    y = 0\n",
    "    if(root.attrib['gender'] == 'male'):\n",
    "        y = 1\n",
    "       \n",
    "    for text in root.findall('conversations/conversation'):\n",
    "        removed_tags = re.sub(r'<[^>]*>', '', str(text.text), flags=re.MULTILINE).lower()\n",
    "        #TODO remove http \n",
    "        #removed_url = re.sub(r';http:[^\\S]*\\.php', '', removed_tags, flags=re.MULTILINE)\n",
    "        #x_train.append(np.array(removed_tags.split()))        \n",
    "        all_texts.append(removed_tags.split())            \n",
    "        y_train.append(y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1543978 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index)) #3940 with the small train set\n",
    "\n",
    "#can use x_train and y_train\n",
    "for text in all_texts:\n",
    "    text_ids = []\n",
    "    for word in text:            \n",
    "        word_id = word_index.get(word, -1)\n",
    "        if word_id != -1:\n",
    "            text_ids.append(word_id)\n",
    "    x_train.append(text_ids)        \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "x = np.array(x_train)\n",
    "\n",
    "#TODO pad sequence length to max_text_length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_text_length, padding='post')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building embedding matrix\n",
    "def get_embedding_layer(embedding_dim):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    #embedding layer\n",
    "    embedding_layer = Embedding(len(word_index) + 1, embedding_dim, weights=[embedding_matrix],\n",
    "                                input_length=max_text_length, trainable=False)\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding_dim, lstm_units, dropout):\n",
    "    embedding_layer = get_embedding_layer(embedding_dim)\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])#try rmsprop\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aleena/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/aleena/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          154397900 \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 154,478,502\n",
      "Trainable params: 80,602\n",
      "Non-trainable params: 154,397,900\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/aleena/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n",
      "163371/163371 [==============================] - 1172s 7ms/step - loss: 0.6893 - acc: 0.5430\n",
      "Epoch 2/3\n",
      "163371/163371 [==============================] - 1075s 7ms/step - loss: 0.6872 - acc: 0.5484\n",
      "Epoch 3/3\n",
      "163371/163371 [==============================] - 895s 5ms/step - loss: 0.6864 - acc: 0.5496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f0ffeaba8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])#try rmsprop\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=512)#TODO try more epochs at least 100\n",
    "model.save('./models/bs_512_em_100_model_3e.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 100)          154397900 \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 154,478,902\n",
      "Trainable params: 80,802\n",
      "Non-trainable params: 154,398,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 55s 5ms/step - loss: 0.8784 - acc: 0.5077\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 50s 5ms/step - loss: 0.7877 - acc: 0.5164\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 47s 5ms/step - loss: 0.7759 - acc: 0.5145\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.7631 - acc: 0.5273\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.7409 - acc: 0.5254\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.7283 - acc: 0.5376\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 45s 5ms/step - loss: 0.7203 - acc: 0.5380\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 45s 5ms/step - loss: 0.7106 - acc: 0.5433\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 47s 5ms/step - loss: 0.7003 - acc: 0.5439\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.6869 - acc: 0.5554\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 45s 5ms/step - loss: 0.6852 - acc: 0.5535\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 53s 5ms/step - loss: 0.6767 - acc: 0.5660\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 47s 5ms/step - loss: 0.6734 - acc: 0.5754\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.6677 - acc: 0.5729\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 45s 5ms/step - loss: 0.6695 - acc: 0.5732\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.6550 - acc: 0.5851\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 51s 5ms/step - loss: 0.6464 - acc: 0.5944\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 56s 6ms/step - loss: 0.6404 - acc: 0.5964\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 51s 5ms/step - loss: 0.6393 - acc: 0.5957\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.6305 - acc: 0.6147\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 54s 5ms/step - loss: 0.6319 - acc: 0.6085\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 47s 5ms/step - loss: 0.6329 - acc: 0.6020\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.6190 - acc: 0.6171\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.6105 - acc: 0.6232\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.6015 - acc: 0.6355\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 47s 5ms/step - loss: 0.5936 - acc: 0.6439\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.5856 - acc: 0.6470\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.5773 - acc: 0.6514\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.5676 - acc: 0.6647\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.5587 - acc: 0.6645\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.5582 - acc: 0.6608\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.5385 - acc: 0.6834\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.5251 - acc: 0.6864\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.5142 - acc: 0.6916\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.5049 - acc: 0.6969\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.4881 - acc: 0.7102\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.4783 - acc: 0.7109\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.4746 - acc: 0.7103\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.4620 - acc: 0.7247\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.4485 - acc: 0.7286\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.4580 - acc: 0.7275\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.4388 - acc: 0.7368\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.4196 - acc: 0.7378\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.4177 - acc: 0.7450\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.4091 - acc: 0.7454\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.3980 - acc: 0.7584\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 45s 5ms/step - loss: 0.3822 - acc: 0.7628\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.3815 - acc: 0.7558\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.3825 - acc: 0.7635\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 0.3797 - acc: 0.7587\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(embedding_layer)\n",
    "model2.add(LSTM(100))\n",
    "model2.add(BatchNormalization())#added\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(2, activation='softmax'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])#try rmsprop\n",
    "print(model2.summary())\n",
    "\n",
    "model2.fit(x_train[:10000], y_train[:10000], epochs=50, batch_size=512)#TODO try more epochs at least 100\n",
    "model2.save('./models/small_bs_512_em_100_model_50e.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/aleena/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/aleena/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "<bound method Network.summary of <keras.engine.sequential.Sequential object at 0x7fd29efeaf28>>\n",
      "WARNING:tensorflow:From /home/aleena/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.6946 - acc: 0.5141 - val_loss: 0.7030 - val_acc: 0.5090\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6915 - acc: 0.5366 - val_loss: 0.6968 - val_acc: 0.5020\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6883 - acc: 0.5400 - val_loss: 0.6980 - val_acc: 0.4985\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.6871 - acc: 0.5460 - val_loss: 0.6993 - val_acc: 0.4970\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6833 - acc: 0.5534 - val_loss: 0.7029 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6817 - acc: 0.5573 - val_loss: 0.7035 - val_acc: 0.4970\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6781 - acc: 0.5693 - val_loss: 0.7232 - val_acc: 0.5020\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6741 - acc: 0.5700 - val_loss: 0.7052 - val_acc: 0.5100\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6711 - acc: 0.5762 - val_loss: 0.7122 - val_acc: 0.5030\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.6639 - acc: 0.5844 - val_loss: 0.7190 - val_acc: 0.5065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd29efea860>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = build_model(embedding_dim, lstm_units=100, dropout=0.2)\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])#try rmsprop\n",
    "print(model3.summary)\n",
    "model3.fit(x_train[:10000], y_train[:10000], epochs=10, validation_split=0.2, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Network.summary of <keras.engine.sequential.Sequential object at 0x7fd29f2cadd8>>\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/150\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.6938 - acc: 0.5274 - val_loss: 0.6948 - val_acc: 0.5020\n",
      "Epoch 2/150\n",
      "8000/8000 [==============================] - 59s 7ms/step - loss: 0.6927 - acc: 0.5246 - val_loss: 0.6952 - val_acc: 0.5050\n",
      "Epoch 3/150\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.6878 - acc: 0.5414 - val_loss: 0.7034 - val_acc: 0.5040\n",
      "Epoch 4/150\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.6860 - acc: 0.5455 - val_loss: 0.7010 - val_acc: 0.4970\n",
      "Epoch 5/150\n",
      "8000/8000 [==============================] - 58s 7ms/step - loss: 0.6847 - acc: 0.5481 - val_loss: 0.7063 - val_acc: 0.5035\n",
      "Epoch 6/150\n",
      "8000/8000 [==============================] - 59s 7ms/step - loss: 0.6826 - acc: 0.5563 - val_loss: 0.7032 - val_acc: 0.5060\n",
      "Epoch 7/150\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.6803 - acc: 0.5614 - val_loss: 0.7054 - val_acc: 0.5110\n",
      "Epoch 8/150\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.6763 - acc: 0.5636 - val_loss: 0.7180 - val_acc: 0.5055\n",
      "Epoch 9/150\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.6735 - acc: 0.5672 - val_loss: 0.7092 - val_acc: 0.5125\n",
      "Epoch 10/150\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.6677 - acc: 0.5785 - val_loss: 0.7213 - val_acc: 0.5080\n",
      "Epoch 11/150\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.6621 - acc: 0.5876 - val_loss: 0.7209 - val_acc: 0.5065\n",
      "Epoch 12/150\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.6619 - acc: 0.5897 - val_loss: 0.7102 - val_acc: 0.5095\n",
      "Epoch 13/150\n",
      "8000/8000 [==============================] - 61s 8ms/step - loss: 0.6542 - acc: 0.6026 - val_loss: 0.7314 - val_acc: 0.5020\n",
      "Epoch 14/150\n",
      "8000/8000 [==============================] - 60s 7ms/step - loss: 0.6430 - acc: 0.6094 - val_loss: 0.7266 - val_acc: 0.5010\n",
      "Epoch 15/150\n",
      "8000/8000 [==============================] - 57s 7ms/step - loss: 0.6333 - acc: 0.6220 - val_loss: 0.7335 - val_acc: 0.5145\n",
      "Epoch 16/150\n",
      "8000/8000 [==============================] - 55s 7ms/step - loss: 0.6205 - acc: 0.6326 - val_loss: 0.7471 - val_acc: 0.5145\n",
      "Epoch 17/150\n",
      "8000/8000 [==============================] - 53s 7ms/step - loss: 0.6054 - acc: 0.6419 - val_loss: 0.7716 - val_acc: 0.5000\n",
      "Epoch 18/150\n",
      "8000/8000 [==============================] - 51s 6ms/step - loss: 0.5852 - acc: 0.6565 - val_loss: 0.7984 - val_acc: 0.5110\n",
      "Epoch 19/150\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.5636 - acc: 0.6730 - val_loss: 0.8006 - val_acc: 0.5145\n",
      "Epoch 20/150\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.5381 - acc: 0.6869 - val_loss: 0.8772 - val_acc: 0.5100\n",
      "Epoch 21/150\n",
      "8000/8000 [==============================] - 47s 6ms/step - loss: 0.5157 - acc: 0.7046 - val_loss: 0.9085 - val_acc: 0.5115\n",
      "Epoch 22/150\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.4915 - acc: 0.7137 - val_loss: 0.9601 - val_acc: 0.5150\n",
      "Epoch 23/150\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.4683 - acc: 0.7270 - val_loss: 1.0148 - val_acc: 0.5210\n",
      "Epoch 24/150\n",
      "8000/8000 [==============================] - 44s 5ms/step - loss: 0.4370 - acc: 0.7423 - val_loss: 1.0327 - val_acc: 0.5070\n",
      "Epoch 25/150\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 0.4171 - acc: 0.7515 - val_loss: 1.0745 - val_acc: 0.5220\n",
      "Epoch 26/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.3997 - acc: 0.7669 - val_loss: 1.1216 - val_acc: 0.5180\n",
      "Epoch 27/150\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 0.3863 - acc: 0.7657 - val_loss: 1.2128 - val_acc: 0.5125\n",
      "Epoch 28/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.3760 - acc: 0.7751 - val_loss: 1.1562 - val_acc: 0.5210\n",
      "Epoch 29/150\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 0.3622 - acc: 0.7824 - val_loss: 1.2905 - val_acc: 0.5155\n",
      "Epoch 30/150\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 0.3442 - acc: 0.7787 - val_loss: 1.4010 - val_acc: 0.5280\n",
      "Epoch 31/150\n",
      "8000/8000 [==============================] - 42s 5ms/step - loss: 0.3345 - acc: 0.7921 - val_loss: 1.4996 - val_acc: 0.5070\n",
      "Epoch 32/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.3293 - acc: 0.7953 - val_loss: 1.4505 - val_acc: 0.5200\n",
      "Epoch 33/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.3297 - acc: 0.7923 - val_loss: 1.5154 - val_acc: 0.5225\n",
      "Epoch 34/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.3251 - acc: 0.7976 - val_loss: 1.5320 - val_acc: 0.5130\n",
      "Epoch 35/150\n",
      "8000/8000 [==============================] - 60s 8ms/step - loss: 0.3269 - acc: 0.7979 - val_loss: 1.5483 - val_acc: 0.5205\n",
      "Epoch 36/150\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.3358 - acc: 0.7874 - val_loss: 1.5071 - val_acc: 0.5210\n",
      "Epoch 37/150\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.3301 - acc: 0.7858 - val_loss: 1.5948 - val_acc: 0.5120\n",
      "Epoch 38/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.3237 - acc: 0.7981 - val_loss: 1.5021 - val_acc: 0.5105\n",
      "Epoch 39/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.3179 - acc: 0.8014 - val_loss: 1.6200 - val_acc: 0.5180\n",
      "Epoch 40/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.3113 - acc: 0.7954 - val_loss: 1.5181 - val_acc: 0.5175\n",
      "Epoch 41/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.3037 - acc: 0.8076 - val_loss: 1.6829 - val_acc: 0.5170\n",
      "Epoch 42/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2988 - acc: 0.8114 - val_loss: 1.7475 - val_acc: 0.5215\n",
      "Epoch 43/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2992 - acc: 0.8062 - val_loss: 1.7365 - val_acc: 0.5105\n",
      "Epoch 44/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2967 - acc: 0.8059 - val_loss: 1.7157 - val_acc: 0.5180\n",
      "Epoch 45/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2984 - acc: 0.8032 - val_loss: 1.8256 - val_acc: 0.5215\n",
      "Epoch 46/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2997 - acc: 0.7993 - val_loss: 1.8000 - val_acc: 0.5055\n",
      "Epoch 47/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2942 - acc: 0.8170 - val_loss: 1.8672 - val_acc: 0.5110\n",
      "Epoch 48/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2941 - acc: 0.8191 - val_loss: 1.8896 - val_acc: 0.5155\n",
      "Epoch 49/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.3044 - acc: 0.8130 - val_loss: 1.7542 - val_acc: 0.5090\n",
      "Epoch 50/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2986 - acc: 0.8157 - val_loss: 1.8345 - val_acc: 0.5130\n",
      "Epoch 51/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2882 - acc: 0.8226 - val_loss: 1.8837 - val_acc: 0.5140\n",
      "Epoch 52/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2901 - acc: 0.8211 - val_loss: 1.8320 - val_acc: 0.5130\n",
      "Epoch 53/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2856 - acc: 0.8274 - val_loss: 1.7764 - val_acc: 0.5180\n",
      "Epoch 54/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2816 - acc: 0.8220 - val_loss: 1.9428 - val_acc: 0.4825\n",
      "Epoch 55/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2787 - acc: 0.8219 - val_loss: 1.8605 - val_acc: 0.5225\n",
      "Epoch 56/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2744 - acc: 0.8251 - val_loss: 1.9451 - val_acc: 0.5040\n",
      "Epoch 57/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2721 - acc: 0.8331 - val_loss: 1.8346 - val_acc: 0.5140\n",
      "Epoch 58/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2750 - acc: 0.8340 - val_loss: 1.9508 - val_acc: 0.5135\n",
      "Epoch 59/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2809 - acc: 0.8307 - val_loss: 1.9929 - val_acc: 0.5075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2700 - acc: 0.8380 - val_loss: 1.9832 - val_acc: 0.5115\n",
      "Epoch 61/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2760 - acc: 0.8356 - val_loss: 1.8704 - val_acc: 0.5200\n",
      "Epoch 62/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2844 - acc: 0.8310 - val_loss: 1.7725 - val_acc: 0.5155\n",
      "Epoch 63/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2827 - acc: 0.8309 - val_loss: 1.9122 - val_acc: 0.5190\n",
      "Epoch 64/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2732 - acc: 0.8382 - val_loss: 1.7865 - val_acc: 0.5155\n",
      "Epoch 65/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2721 - acc: 0.8379 - val_loss: 1.8507 - val_acc: 0.5085\n",
      "Epoch 66/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2632 - acc: 0.8430 - val_loss: 1.7915 - val_acc: 0.5200\n",
      "Epoch 67/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2638 - acc: 0.8411 - val_loss: 1.9592 - val_acc: 0.5120\n",
      "Epoch 68/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2740 - acc: 0.8358 - val_loss: 1.9569 - val_acc: 0.5145\n",
      "Epoch 69/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2585 - acc: 0.8455 - val_loss: 1.9969 - val_acc: 0.5135\n",
      "Epoch 70/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2504 - acc: 0.8486 - val_loss: 1.9833 - val_acc: 0.5125\n",
      "Epoch 71/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2451 - acc: 0.8525 - val_loss: 2.2333 - val_acc: 0.5185\n",
      "Epoch 72/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2488 - acc: 0.8539 - val_loss: 2.1355 - val_acc: 0.5150\n",
      "Epoch 73/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2559 - acc: 0.8481 - val_loss: 2.0288 - val_acc: 0.5235\n",
      "Epoch 74/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2536 - acc: 0.8490 - val_loss: 2.0266 - val_acc: 0.5110\n",
      "Epoch 75/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2497 - acc: 0.8535 - val_loss: 2.1674 - val_acc: 0.5050\n",
      "Epoch 76/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2555 - acc: 0.8500 - val_loss: 1.9417 - val_acc: 0.5280\n",
      "Epoch 77/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2527 - acc: 0.8369 - val_loss: 2.0784 - val_acc: 0.5175\n",
      "Epoch 78/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2499 - acc: 0.8546 - val_loss: 2.0524 - val_acc: 0.5195\n",
      "Epoch 79/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2483 - acc: 0.8556 - val_loss: 1.9405 - val_acc: 0.5170\n",
      "Epoch 80/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2512 - acc: 0.8502 - val_loss: 2.0492 - val_acc: 0.5300\n",
      "Epoch 81/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2522 - acc: 0.8504 - val_loss: 2.0186 - val_acc: 0.5250\n",
      "Epoch 82/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2738 - acc: 0.8357 - val_loss: 1.9287 - val_acc: 0.5225\n",
      "Epoch 83/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2546 - acc: 0.8496 - val_loss: 2.1906 - val_acc: 0.5200\n",
      "Epoch 84/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2411 - acc: 0.8561 - val_loss: 2.0685 - val_acc: 0.5235\n",
      "Epoch 85/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2411 - acc: 0.8598 - val_loss: 1.9987 - val_acc: 0.5225\n",
      "Epoch 86/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2450 - acc: 0.8578 - val_loss: 2.3222 - val_acc: 0.5250\n",
      "Epoch 87/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2292 - acc: 0.8646 - val_loss: 2.2088 - val_acc: 0.5145\n",
      "Epoch 88/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2288 - acc: 0.8660 - val_loss: 2.2344 - val_acc: 0.5200\n",
      "Epoch 89/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2329 - acc: 0.8636 - val_loss: 2.0963 - val_acc: 0.5240\n",
      "Epoch 90/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2379 - acc: 0.8606 - val_loss: 2.0605 - val_acc: 0.5230\n",
      "Epoch 91/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2374 - acc: 0.8639 - val_loss: 2.1922 - val_acc: 0.5175\n",
      "Epoch 92/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2483 - acc: 0.8555 - val_loss: 2.0482 - val_acc: 0.5270\n",
      "Epoch 93/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2464 - acc: 0.8541 - val_loss: 2.1726 - val_acc: 0.5290\n",
      "Epoch 94/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2293 - acc: 0.8580 - val_loss: 2.1094 - val_acc: 0.5265\n",
      "Epoch 95/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2324 - acc: 0.8647 - val_loss: 2.1263 - val_acc: 0.5230\n",
      "Epoch 96/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2329 - acc: 0.8655 - val_loss: 2.0550 - val_acc: 0.5260\n",
      "Epoch 97/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2368 - acc: 0.8621 - val_loss: 2.2161 - val_acc: 0.5265\n",
      "Epoch 98/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2301 - acc: 0.8657 - val_loss: 2.3563 - val_acc: 0.5245\n",
      "Epoch 99/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2202 - acc: 0.8720 - val_loss: 2.2662 - val_acc: 0.5295\n",
      "Epoch 100/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2269 - acc: 0.8648 - val_loss: 2.0907 - val_acc: 0.5290\n",
      "Epoch 101/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2260 - acc: 0.8674 - val_loss: 2.2812 - val_acc: 0.5165\n",
      "Epoch 102/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2463 - acc: 0.8599 - val_loss: 2.2621 - val_acc: 0.5070\n",
      "Epoch 103/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2645 - acc: 0.8483 - val_loss: 2.1419 - val_acc: 0.5145\n",
      "Epoch 104/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2453 - acc: 0.8608 - val_loss: 2.1033 - val_acc: 0.5170\n",
      "Epoch 105/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2307 - acc: 0.8645 - val_loss: 2.3878 - val_acc: 0.5195\n",
      "Epoch 106/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2429 - acc: 0.8574 - val_loss: 1.9743 - val_acc: 0.5280\n",
      "Epoch 107/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2381 - acc: 0.8629 - val_loss: 2.2981 - val_acc: 0.5275\n",
      "Epoch 108/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2213 - acc: 0.8708 - val_loss: 2.2735 - val_acc: 0.5305\n",
      "Epoch 109/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2247 - acc: 0.8697 - val_loss: 2.3400 - val_acc: 0.5200\n",
      "Epoch 110/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2394 - acc: 0.8666 - val_loss: 2.4123 - val_acc: 0.5090\n",
      "Epoch 111/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2225 - acc: 0.8709 - val_loss: 2.3773 - val_acc: 0.5170\n",
      "Epoch 112/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2178 - acc: 0.8728 - val_loss: 2.3773 - val_acc: 0.5275\n",
      "Epoch 113/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2454 - acc: 0.8645 - val_loss: 2.2974 - val_acc: 0.5170\n",
      "Epoch 114/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2304 - acc: 0.8675 - val_loss: 2.1938 - val_acc: 0.5205\n",
      "Epoch 115/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2422 - acc: 0.8624 - val_loss: 2.0748 - val_acc: 0.5270\n",
      "Epoch 116/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2558 - acc: 0.8502 - val_loss: 2.3475 - val_acc: 0.5160\n",
      "Epoch 117/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2222 - acc: 0.8734 - val_loss: 2.3644 - val_acc: 0.5225\n",
      "Epoch 118/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2258 - acc: 0.8729 - val_loss: 2.2500 - val_acc: 0.5175\n",
      "Epoch 119/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2320 - acc: 0.8669 - val_loss: 2.2442 - val_acc: 0.5210\n",
      "Epoch 120/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2285 - acc: 0.8654 - val_loss: 2.4937 - val_acc: 0.5255\n",
      "Epoch 121/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2449 - acc: 0.8632 - val_loss: 2.2566 - val_acc: 0.5270\n",
      "Epoch 122/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2257 - acc: 0.8761 - val_loss: 2.2427 - val_acc: 0.5165\n",
      "Epoch 123/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2148 - acc: 0.8776 - val_loss: 2.4101 - val_acc: 0.5190\n",
      "Epoch 124/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2060 - acc: 0.8825 - val_loss: 2.4747 - val_acc: 0.5190\n",
      "Epoch 125/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2015 - acc: 0.8839 - val_loss: 2.4151 - val_acc: 0.5110\n",
      "Epoch 126/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2192 - acc: 0.8744 - val_loss: 2.3552 - val_acc: 0.5145\n",
      "Epoch 127/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2213 - acc: 0.8734 - val_loss: 2.5760 - val_acc: 0.5030\n",
      "Epoch 128/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2047 - acc: 0.8839 - val_loss: 2.3880 - val_acc: 0.5155\n",
      "Epoch 129/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2037 - acc: 0.8806 - val_loss: 2.4980 - val_acc: 0.5130\n",
      "Epoch 130/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.1901 - acc: 0.8902 - val_loss: 2.5337 - val_acc: 0.5205\n",
      "Epoch 131/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.1940 - acc: 0.8890 - val_loss: 2.3847 - val_acc: 0.5170\n",
      "Epoch 132/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.1993 - acc: 0.8870 - val_loss: 2.3077 - val_acc: 0.5200\n",
      "Epoch 133/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2104 - acc: 0.8805 - val_loss: 2.1575 - val_acc: 0.5140\n",
      "Epoch 134/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2198 - acc: 0.8744 - val_loss: 2.6222 - val_acc: 0.5175\n",
      "Epoch 135/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2215 - acc: 0.8750 - val_loss: 2.3972 - val_acc: 0.5040\n",
      "Epoch 136/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2173 - acc: 0.8781 - val_loss: 2.5099 - val_acc: 0.5115\n",
      "Epoch 137/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2059 - acc: 0.8825 - val_loss: 2.4506 - val_acc: 0.5275\n",
      "Epoch 138/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2105 - acc: 0.8850 - val_loss: 2.3951 - val_acc: 0.5110\n",
      "Epoch 139/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2176 - acc: 0.8811 - val_loss: 2.4461 - val_acc: 0.5180\n",
      "Epoch 140/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.1975 - acc: 0.8880 - val_loss: 2.4519 - val_acc: 0.5255\n",
      "Epoch 141/150\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.1878 - acc: 0.8911 - val_loss: 2.5030 - val_acc: 0.5140\n",
      "Epoch 142/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.1856 - acc: 0.8938 - val_loss: 2.6749 - val_acc: 0.5205\n",
      "Epoch 143/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.1825 - acc: 0.8981 - val_loss: 2.6758 - val_acc: 0.5200\n",
      "Epoch 144/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2014 - acc: 0.8876 - val_loss: 2.6032 - val_acc: 0.5190\n",
      "Epoch 145/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2052 - acc: 0.8825 - val_loss: 2.6694 - val_acc: 0.5105\n",
      "Epoch 146/150\n",
      "8000/8000 [==============================] - 43s 5ms/step - loss: 0.2300 - acc: 0.8699 - val_loss: 2.5126 - val_acc: 0.5150\n",
      "Epoch 147/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2106 - acc: 0.8780 - val_loss: 2.4306 - val_acc: 0.5145\n",
      "Epoch 148/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2022 - acc: 0.8846 - val_loss: 2.5889 - val_acc: 0.5145\n",
      "Epoch 149/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2886 - acc: 0.8476 - val_loss: 1.9465 - val_acc: 0.5130\n",
      "Epoch 150/150\n",
      "8000/8000 [==============================] - 41s 5ms/step - loss: 0.2723 - acc: 0.8572 - val_loss: 2.1741 - val_acc: 0.5155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd282285208>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = build_model(embedding_dim, lstm_units=100, dropout=0.2)\n",
    "model4.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])#try rmsprop\n",
    "print(model4.summary)\n",
    "model4.fit(x_train[:10000], y_train[:10000], epochs=150, validation_split=0.2, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.190560478210449, 0.525]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.evaluate(x_train[-1000:], y_train[-1000:], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5ad219269bc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#try rmsprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_model' is not defined"
     ]
    }
   ],
   "source": [
    "model5 = build_model(embedding_dim, lstm_units=256, dropout=0.2)\n",
    "model5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])#try rmsprop\n",
    "model5.fit(x_train[:5000], y_train[:5000], epochs=200, validation_split=0.1, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
